{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNvGxWS6kOpS"
   },
   "source": [
    "# Task 4.2: Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbPOUZNFSPKN"
   },
   "source": [
    "## 4.2.1 - Principal Component Analysis (PCA)\n",
    "\n",
    "Data is one of the main drivers behind most of Machine Learning applications. Analysing the data prior to any usage is of crucial importance, because data can be complicated and it may be challenging to understand what it all means and which parts actually carry information.\n",
    "\n",
    "Dimensionality reduction is a technique which helps us gain a better macro-level understanding of the data by reducing the number of features that describe a dataset such that only the most information-rich parts remain.\n",
    "\n",
    "Principal Component Analysis (PCA) is a simple yet powerful technique used for dimensionality reduction. Through it, we can directly decrease the number of feature variables, thereby narrowing down the important features and saving on computations. From a high-level view PCA has three main steps:\n",
    "\n",
    "1. Compute the **covariance matrix** of the data\n",
    "2. Compute the **eigen values** and **vectors** of this covariance matrix\n",
    "3. Use the eigen values and vectors to select only the most important feature dimensions that carry enough information to approximate the original data.\n",
    "\n",
    "\n",
    "If you are interested to know more about the PCA steps, check [this](https://plot.ly/python/v3/ipython-notebooks/principal-component-analysis/) useful tutorial in NumPy.\n",
    "\n",
    "\n",
    "The entire process will be illustrated in the script below, using the same [Meteonorm](https://meteonorm.com/en/download) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Kqw6joVZSPKP"
   },
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Qdx-d-erSPKY"
   },
   "outputs": [],
   "source": [
    "#@title Load data\n",
    "df1 = pd.read_table('./weather-data/Bern-hour.dat', sep=',', header=None)\n",
    "df2 = pd.read_table('./weather-data/Johannesburg-hour.dat', sep=',', header=None)\n",
    "df3 = pd.read_table('./weather-data/SanFrancisco-hour.dat', sep=',', header=None)\n",
    "df4 = pd.read_table('./weather-data/Perth-hour.dat', sep=',', header=None)\n",
    "df5 = pd.read_table('./weather-data/Brasilia-hour.dat', sep=',', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "BITRAOFgSPKe"
   },
   "outputs": [],
   "source": [
    "#@title Helper function\n",
    "def create_dataframe(dataframe, cls):\n",
    "    \n",
    "    import datetime as dt\n",
    "    \n",
    "    dataframe.columns = ['year', 'month', 'day', 'hour', 'global radiation', 'diffuse radiation', \n",
    "                         'temp', 'wind speed','relative humidity', 'cloud cover', 'precipitation']\n",
    "    \n",
    "    datetime = dataframe.loc[:, 'year':'hour']\n",
    "    \n",
    "    # the original data has hours in the 1 --> 24 format, but datetime accepts only 0 --> 23\n",
    "    datetime['hour'] = datetime['hour'] - 1\n",
    "    \n",
    "    datetime['DateTime'] = datetime.apply(lambda row: dt.datetime(row.year, row.month, row.day, row.hour), axis=1)\n",
    "    datetime['DateTime'] = pd.to_datetime(datetime.DateTime)\n",
    "    dataframe.index = pd.DatetimeIndex(datetime.DateTime)\n",
    "    \n",
    "    # include the class column for each city\n",
    "    dataframe['class'] = np.full(shape=dataframe.shape[0], fill_value=cls)\n",
    "    \n",
    "    # delete the first four columns, they are not needed now that there is a DateTimeIndex\n",
    "    dataframe = dataframe.drop(['year', 'month', 'day', 'hour'], axis=1)\n",
    "    \n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "CGtQYWm2SPKj"
   },
   "outputs": [],
   "source": [
    "#@title Preprocess data frames\n",
    "df_bern = create_dataframe(df1, cls=0)\n",
    "df_perth = create_dataframe(df4, cls=1)\n",
    "df_johannesburg = create_dataframe(df2, cls=2)\n",
    "df_sanfrancisco = create_dataframe(df3, cls=3)\n",
    "df_brasilia = create_dataframe(df5, cls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6CoUGBcSPKm"
   },
   "source": [
    "## 4.2.1.1. Applying PCA to two cities\n",
    "\n",
    "As an example, we have decided to apply the PCA to the cities of Perth and Bern, since they belong to the Southern and Northern hemisphere, respectively. Owing to the tilt of Earth's rotation relative to the Sun and the ecliptic plane, the seasons are basically inverted between the two hemispheres. \n",
    "\n",
    "Therefore, we expect to find clear differences in weather data by extracting only the four months in which it is winter in Bern and summer in Perth, i.e. January, February, November and December. Via PCA, we can reduce the dimensionality of the dataset by projecting the whole data **into the few dimensions that are necessary to discriminated between the two hemispheres**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_eem_XeSPKn",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we can take the desired period by dropping the months in the middle (i.e. March to October)\n",
    "\n",
    "df_perth_summer = df_perth.drop(df_perth.loc['2005-03-01 00:00:00':'2005-10-31 23:00:00', :].index, axis=0)\n",
    "\n",
    "df_perth_summer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EFUFqsHbSPKr"
   },
   "outputs": [],
   "source": [
    "df_bern_winter = df_bern.drop(df_bern.loc['2005-03-01 00:00:00':'2005-10-31 23:00:00', :].index, axis=0)\n",
    "\n",
    "df_bern_winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h7ZaM82RSPKu"
   },
   "outputs": [],
   "source": [
    "# we now combined the data of both cities for our PCA analysis\n",
    "df_pca = pd.concat([df_bern_winter, df_perth_summer], keys=['Bern','Perth'], axis=0, join='inner')\n",
    "\n",
    "df_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8ksPSKySPKw"
   },
   "source": [
    "First, we take all the weather parameters (X) as inputs to the PCA algorithm. The target (y) is the city identifier, *class*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HJmMVLBpSPKx"
   },
   "outputs": [],
   "source": [
    "X = df_pca.loc[:, 'global radiation':'precipitation'].values\n",
    "y = df_pca.loc[:, 'class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmfUfceNSPKz"
   },
   "source": [
    "PCA returns a sub-space that maximizes the variance along the feature vectors. Therefore, in order to properly measure the variance of those feature vectors, they must be equally balanced. To do this, we first normalise our data to have zero-mean and unit-variance such that each feature will be weighted equally in our calculations. \n",
    "\n",
    "This is done by using the [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) module in **scikit-learn**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1BNo1EoSPKz"
   },
   "outputs": [],
   "source": [
    "# the data is scaled \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_std = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyoE99NHSPK1"
   },
   "source": [
    "scikit-learn implements a module for computing the PCA: `fit_transform()`. You can specify the desired number of principal components or you can take the ones that hold most of the variance (or information) about the dataset. Depending on the problem and the data, the first 2 or 3 principal components can be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKdtDC3dSPK2"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "Y = pca.fit_transform(X_std)\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwp_UbQLSPK4"
   },
   "source": [
    "How much of the variance is explained by the first two principal components?\n",
    "Let's see whether or not it is enough to clearly discriminated between Bern and Perth. \n",
    "\n",
    "Here is an example of how the data looks like for the first principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jlbtRBfaSPK4"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.scatter(Y[:, 0][y == 0], np.zeros((Y[y == 0].shape[0], 1)), marker='o', s=50, label='Bern')\n",
    "ax.scatter(Y[:, 0][y == 1], np.zeros((Y[y == 1].shape[0], 1)), marker='x', s=10, label='Perth')\n",
    "\n",
    "ax.set_xlim([-6, 6])\n",
    "ax.set_ylim([-1, 1])\n",
    "\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_title('Scatter plot for the cities of Bern and Perth - Jan, Feb, Nov and Dec')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Fv3fPRBSPK6"
   },
   "source": [
    "You can see that there is a big overlap between the two classes (cities). \n",
    "\n",
    "**[TO DO]:** What happens if you plot the first two principal components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Szwb6gQMSPK7"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Scatterplot with the first two principal components\n",
    "###########################\n",
    "\n",
    "\n",
    "### TO DO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMm856Z3SPK9"
   },
   "source": [
    "**[Optional]:** Try to make a 3-D scatterplot using the three principal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PpdyJxj6SPK9",
    "outputId": "df5f48f4-bbb7-4fd4-c2fb-42d345e78b41"
   },
   "outputs": [],
   "source": [
    "# how to generate 3d figures using matplotlib \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "### TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGrwkMO11H9z"
   },
   "source": [
    "## 4.2.2 - Non Linear Dimensionality Reduction \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRND-_sA27V2"
   },
   "source": [
    "What happens when your data are linearly inseparable? Was the data that we previously applied PCA to actually linearly separable? Remember that PCA is a linear projection technique.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzAIwiN13XOP"
   },
   "source": [
    "### 4.2.2.1 Kernel Principal Component Analysis (K-PCA)\n",
    "\n",
    "The first approach we'll look at is Kernel PCA which uses a non-linear kernel function with standard PCA to achieve non-linear dimensionality reduction. In practice, it projects the linearly inseparable data into a higher dimensional space that can be linearly separated using PCA. The main steps of KPCA are:\n",
    "1. Select a non-linear kernel mapping\n",
    "2. Construct the normalized kernel matrix\n",
    "3. Find the eigen values and vectors of this matrix\n",
    "4. For each data point, use the eigen values and vectors to obtain its principal components.\n",
    "\n",
    "Most common kernel functions are Gaussian, Sigmoid and Polynomial. You can learn more details about Kernel PCA [here](http://axon.cs.byu.edu/~martinez/classes/778/Papers/KernelPCA.pdf). You might also find it useful to read about [Kernel SVM](https://scikit-learn.org/stable/modules/svm.html) since it has many conceptual similarities to  Kernel PCA. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MDQp9kE3sWt"
   },
   "source": [
    "We use the same dataset as in PCA example. We will apply the Kernal PCA to the cities of Perth and Johannesburg and we are going to use only January, February, November and December which it is summer both in Perth and  Johannesburg. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "z7-R98WJ57Kp"
   },
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "7sN4qbBv58kH"
   },
   "outputs": [],
   "source": [
    "#@title Load and preprocess data as in PCA but just for 'Perth' and 'Johannesburg'\n",
    "df2 = pd.read_table('./weather-data/Johannesburg-hour.dat', sep=',', header=None)\n",
    "df4 = pd.read_table('./weather-data/Perth-hour.dat', sep=',', header=None)\n",
    "\n",
    "# helper function\n",
    "def create_dataframe(dataframe, cls):\n",
    "    \n",
    "    import datetime as dt\n",
    "    \n",
    "    dataframe.columns = ['year', 'month', 'day', 'hour', 'global radiation', 'diffuse radiation', \n",
    "                         'temp', 'wind speed','relative humidity', 'cloud cover', 'precipitation']\n",
    "    \n",
    "    datetime = dataframe.loc[:, 'year':'hour']\n",
    "    \n",
    "    # the original data has hours in the 1 --> 24 format, but datetime accepts only 0 --> 23\n",
    "    datetime['hour'] = datetime['hour'] - 1\n",
    "    \n",
    "    datetime['DateTime'] = datetime.apply(lambda row: dt.datetime(row.year, row.month, row.day, row.hour), axis=1)\n",
    "    datetime['DateTime'] = pd.to_datetime(datetime.DateTime)\n",
    "    dataframe.index = pd.DatetimeIndex(datetime.DateTime)\n",
    "    \n",
    "    # include the class column for each city\n",
    "    dataframe['class'] = np.full(shape=dataframe.shape[0], fill_value=cls)\n",
    "    \n",
    "    # delete the first four columns, they are not needed now that there is a DateTimeIndex\n",
    "    dataframe = dataframe.drop(['year', 'month', 'day', 'hour'], axis=1)\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "df_perth = create_dataframe(df4, cls=1)\n",
    "df_johannesburg = create_dataframe(df2, cls=2)\n",
    "# we can take the desired period by dropping the months in the middle (i.e. March to October)\n",
    "\n",
    "df_perth_summer = df_perth.drop(df_perth.loc['2005-03-01 00:00:00':'2005-10-31 23:00:00', :].index, axis=0)\n",
    "df_johannesburg_summer = df_johannesburg.drop(df_johannesburg.loc['2005-03-01 00:00:00':'2005-10-31 23:00:00', :].index, axis=0)\n",
    "\n",
    "# we now combined the data of both cities for our PCA analysis\n",
    "df_pca = pd.concat([df_perth_summer,df_johannesburg_summer], keys=['Perth','Johannesburg'], axis=0, join='inner')\n",
    "\n",
    "df_pca\n",
    "\n",
    "X = df_pca.loc[:, 'global radiation':'precipitation'].values\n",
    "y = df_pca.loc[:, 'class'].values\n",
    "\n",
    "# the data is scaled \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_std = StandardScaler().fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfYXywxf6xri"
   },
   "source": [
    "Similar to PCA, scikit-learn provides `fit_transform()` for calculating Kernel PCA. You can specify different arguments such as the number of principal components, the kernel function and parameters related to them. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JiPiUPSD26_q"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "kpca =  KernelPCA(kernel='rbf')\n",
    "Y_k = kpca.fit_transform(X_std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JaqOl3lc7Znh"
   },
   "source": [
    "Compute the explained variance and ratio since kernel PCA does not have a built-in function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8kTUVhqx3dFX"
   },
   "outputs": [],
   "source": [
    "explained_variance = np.var(Y_k, axis=0)\n",
    "explained_variance_ratio = explained_variance / np.sum(explained_variance)\n",
    "explained_variance_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPg0zr2F7Bio"
   },
   "source": [
    " **[TO DO]:** Use PCA to separate the data and plot the first three principal components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akFawrjT7Dl5"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Use PCA for dimensionality reduction\n",
    "###########################\n",
    "\n",
    "\n",
    "### TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IC299m4k7D9D"
   },
   "source": [
    " **[TO DO]:** Experiment with the following kernel functions such as `poly`, `sigmoid`, `cosine`, `precomputed`. Visualize the results of each kernel option. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NjwBsv8t7EQV"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Use kernel=poly, cosine, precomputed in KernelPCA \n",
    "###########################\n",
    "\n",
    "\n",
    "### TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7umwQf0p7K6u"
   },
   "source": [
    "**[TO DO]:** What happens when you select a linear kernel? Can you compare the results to standard PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7E94Jgq7LEp"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Use kernel=linear in KernelPCA \n",
    "###########################\n",
    "\n",
    "\n",
    "### TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHDadfzfRb9D"
   },
   "source": [
    "### 4.2.2.2 t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "t-Distributed Stochastic Neighbor Embedding (t-SNE) is an non-linear approach for dimensionality reduction used for exploring and visualizing high-dimensional data. It uses gradient descent to minimize the Kullback-Leibler divergence between the distribution that measures pairwise similarities of the original high-dimensional inputs and the distribution that measures pairwise similarities of the corresponding embedded in a lower-dimension points.\n",
    "\n",
    "You can find a summary of the algorithmic steps [here](https://uk.mathworks.com/help/stats/t-sne.html) or more details in the [original paper](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf).\n",
    "\n",
    "There are quite few hyperparameters to control during optimization process (e.g. perplexity, learning rate, number of iterations etc.). You can find a very informative guideon how to deal with different situations on [How to Use t-SNE Effectively](https://distill.pub/2016/misread-tsne/ )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfpP5EG2RdRd"
   },
   "source": [
    "The `sklearn.manifold` module implements the t-distributed Stochastic Neighbor Embedding. We apply the t-SNE on the data we defined in 2.2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ng4YFYIPRfo_"
   },
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "tsne = manifold.TSNE(n_components=2, init='random',\n",
    "                     random_state=0, perplexity=5)\n",
    "Y = tsne.fit_transform(X)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9ZW1p9uRiSJ"
   },
   "source": [
    "**[TO DO]:** Visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v5ry6I3uRqmv"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   3D Scatterplot with the transformed output of t-SNE\n",
    "###########################\n",
    "\n",
    "\n",
    "### TO DO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afJD4MoQRtPS"
   },
   "source": [
    "**[TO DO]:** Increase the perplexity a lot. What do you notice? What happens when it is higher that the data points? You can try other changes in the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnfwDPn_RtZ2"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Change perplexity parameter\n",
    "###########################\n",
    "\n",
    "\n",
    "### TO DO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nsoI6HgmRtiz"
   },
   "source": [
    "**[OPTIONAL]:** Compare t-SNE results to PCA. What are the advantages and disadvantages of each approach? What happens if you use PCA before t-SNE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TxkDh-1JRtt8"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Use PCA for dimensionality reduction\n",
    "###########################\n",
    "\n",
    "\n",
    "### TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwPCCNfwR4LA"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   3D Scatterplot with the results of PCA\n",
    "###########################\n",
    "\n",
    "\n",
    "### TO DO\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LAB4.2.ipynb",
   "provenance": [
    {
     "file_id": "1N8fnql2TlAKh3ll5S-SHLoU666q8sh-_",
     "timestamp": 1604309140719
    },
    {
     "file_id": "1Oi_asqq_7A12-Rbgi3Mf1NH4TMradDpC",
     "timestamp": 1603822785138
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

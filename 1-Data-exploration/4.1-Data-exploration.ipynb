{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wu_6K_lHkYp9"
   },
   "source": [
    "# Task 4.1 : Data Exploration\n",
    "\n",
    "## 4.1.1 - Get to know your Data\n",
    "\n",
    "Before doing any (pre)processing, it is important to get familiar with your data,know your data and find relation between data.\n",
    "\n",
    "### Data  Objects and attributes\n",
    "Python is an object oriented programming language, this means objects are Python’s abstraction for data. All data in a Python program is represented by objects or by relations between objects.This notebook we wil use the class **DataFrame** to define our data objects.\n",
    "\n",
    "A data object represents the entity. Data Objects are like group of attributes of a entity. \n",
    "A attribute can be seen as a data field that represent characteristics, features or variables or a data object.\n",
    "The attributes can be subdivided in two types\n",
    "\n",
    "**Types of Attributes:**\n",
    "* **Qualitative attributes:** describe an object feature, without providing a measurable size/quantitive/numeric value \n",
    "     )\n",
    "     \n",
    "* **Quantitative attributes:** measurable quantitative representing the object attribute \n",
    "\n",
    "We will load two datasets, one containing weather data and the other containing city details information.\n",
    "The entire process will be illustrated in the script below, where we download weather data using Meteonorm. Meteonorm is a software that collects accurate weather data and representative typical years for any place on earth. You can choose from more than 30 different weather parameters. The DEMO version allows us to download data for only 5 cities in 2005: Bern, Johannesburg, San Francisco, Perth and Brasilia.\n",
    "\n",
    "**City Weather Data - Quantitative attributes**\n",
    "\n",
    "The data is stored in comma-separated files called *city*-hour.dat. \n",
    "Here we have selected the following 7 main weather parameters:\n",
    "- Global radiation ($W/m^2$)\n",
    "- Diffuse radiation ($W/m^2$)\n",
    "- Temperature ($°C$)\n",
    "- Wind speed ($m/s$)\n",
    "- Relative humidity ($\\%$)\n",
    "- Cloud cover ($oktas$)\n",
    "- Precipitation ($mm$)\n",
    "\n",
    "**City Details Data - Qualitative attributes**\n",
    "\n",
    "We load a second data, with the following parameters:\n",
    "- City Name \n",
    "- Country\n",
    "- Language\n",
    "- Climate\n",
    "- Cost of living\n",
    "- Main Sport\n",
    "- Florestation\n",
    "- Hemesphire\n",
    "\n",
    "\n",
    "**NOTE 1** In meteorology, an okta is a unit of measurement used to describe the amount of cloud cover. \n",
    "SKC = Sky clear (0 oktas); FEW = Few (1 to 2 oktas); SCT = Scattered (3 to 4 oktas); BKN = Broken (5 to 7 oktas); OVC = Overcast (8 oktas)\n",
    "\n",
    "**NOTE 2** City details data was filled with online sources, the information may not be updated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "BeTT0AtkkYp9"
   },
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N43ONDsgkYp9"
   },
   "outputs": [],
   "source": [
    "#Create City Details data\n",
    "City_Name = ['Bern','San Francisco','Johannesburg','Perth', 'Brasilia' ]\n",
    "Country = ['Switzerland','United States of America', 'South Africa','Australia', 'Brasil' ]\n",
    "Language = ['German','English', 'English','English', 'Portuguese' ]\n",
    "Climate = ['Continental', 'Mediterranean','SubTropical Highland','Mediterranean','Tropical Savanna']\n",
    "Cost_of_living = ['$$$', '$$$','$','$$','$']\n",
    "Main_Sport = ['Soccer','Basketball', 'Football','Football', 'Soccer' ]\n",
    "Florestation = ['Pines', 'Oak','Baobab','Eucalypt','Pernambuco']\n",
    "Hemesphire = ['North','North','South', 'South', 'South']\n",
    "\n",
    "\n",
    "columns = ['City Name', 'Country' , 'Language','Climate' 'Main Sport', 'Cost of living', 'Florestation','Hemesphire']\n",
    "d = {'City Name': City_Name, 'Country': Country, 'Language': Language, 'Climate':Climate, 'Main Sport': Main_Sport, 'Cost of living':Cost_of_living, 'Florestation':Florestation, 'Hemesphire':Hemesphire }\n",
    "\n",
    "CityDataset = pd.DataFrame(data=d)\n",
    "CityDataset.to_csv(\"./weather-data/CityDataset.dat\", index=False, header = True, sep = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwqjHVgLvw0w"
   },
   "source": [
    "`create_dataframe()` is a function that assigns names to the imported dataset's columns and creates a DateTimeIndex for better manipulation of the time-series data. The function also creates and additional **class** column to distinguish each city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FVtE3L7MkYp9"
   },
   "outputs": [],
   "source": [
    "#@title Helper function\n",
    "def create_dataframe(dataframe, cls):\n",
    "    \n",
    "    import datetime as dt\n",
    "    \n",
    "    dataframe.columns = ['year', 'month', 'day', 'hour', 'global radiation', 'diffuse radiation', \n",
    "                         'temp', 'wind speed','relative humidity', 'cloud cover', 'precipitation']\n",
    "    \n",
    "    datetime = dataframe.loc[:, 'year':'hour']\n",
    "    \n",
    "    # the original data has hours in the 1 --> 24 format, but datetime accepts only 0 --> 23\n",
    "    datetime['hour'] = datetime['hour'] - 1\n",
    "    \n",
    "    datetime['DateTime'] = datetime.apply(lambda row: dt.datetime(row.year, row.month, row.day, row.hour), axis=1)\n",
    "    datetime['DateTime'] = pd.to_datetime(datetime.DateTime)\n",
    "    dataframe.index = pd.DatetimeIndex(datetime.DateTime)\n",
    "    \n",
    "    # include the class column for each city\n",
    "    dataframe['class'] = np.full(shape=dataframe.shape[0], fill_value=cls)\n",
    "    \n",
    "    # delete the first four columns, they are not needed now that there is a DateTimeIndex\n",
    "    dataframe = dataframe.drop(['year', 'month', 'day', 'hour'], axis=1)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kk-j0rgrkYp9"
   },
   "outputs": [],
   "source": [
    "#@title Load Weather data\n",
    "df1 = pd.read_table('./weather-data/Bern-hour.dat', sep=',', header=None)\n",
    "df2 = pd.read_table('./weather-data/Johannesburg-hour.dat', sep=',', header=None)\n",
    "df3 = pd.read_table('./weather-data/SanFrancisco-hour.dat', sep=',', header=None)\n",
    "df4 = pd.read_table('./weather-data/Perth-hour.dat', sep=',', header=None)\n",
    "df5 = pd.read_table('./weather-data/Brasilia-hour.dat', sep=',', header=None)\n",
    "df1 = pd.read_table('./weather-data/Bern-hour.dat', sep=',', header=None)\n",
    "\n",
    "bern_weather = create_dataframe(df1, cls=0)\n",
    "perth_weather = create_dataframe(df4, cls=1)\n",
    "johannesburg_weather = create_dataframe(df2, cls=2)\n",
    "sanfrancisco_weather = create_dataframe(df3, cls=3)\n",
    "brasilia_weather = create_dataframe(df5, cls=4)ß\n",
    "\n",
    "#Load City Details data  for simplicity\n",
    "CityDataset = pd.read_table('./weather-data/CityDataset.dat', sep=',')\n",
    "bern_details = CityDataset.loc[CityDataset['City Name'] == 'Bern' ]\n",
    "sanfrancisco_details = CityDataset.loc[CityDataset['City Name'] == 'San Francisco' ]\n",
    "johannesburg_details = CityDataset.loc[CityDataset['City Name'] == 'Johannesburg' ]\n",
    "perth_details = CityDataset.loc[CityDataset['City Name'] == 'Perth' ]\n",
    "brasilia_details = CityDataset.loc[CityDataset['City Name'] == 'Brasilia' ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9j5BNFbkYp9"
   },
   "source": [
    "For each city, we have two data\n",
    "objects **'City'_weather'** and  **'City'_details** containing city weather and city details respectively.\n",
    "\n",
    "Display the data and get familiar with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qm2lCmmOkYp-"
   },
   "outputs": [],
   "source": [
    "#Visualize our data\n",
    "print(\"Bern details\\n\",bern_details )\n",
    "\n",
    "print(\"\\nBern weather\\n\",bern_weather )\n",
    "\n",
    "#Visualize data using dtypes\n",
    "print(\"\\nBern city details dtypes\")\n",
    "print (bern_details.dtypes)\n",
    "\n",
    "print(\"\\nBern city weather dtypes\")\n",
    "print(bern_weather.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KTTVIFskYp-"
   },
   "source": [
    "## Task 4.1.2 -  Similarity\n",
    "Similarity is the measure of how alike two data objects are. It is a very importnat concept in data exploration and a basic block of unsupervised learning like clustering classification etc.\n",
    "\n",
    "However similarity is subjective and is highly dependent on the domain and application. For example, one could could say two cities, like Perth and San Francisco are similar because both speak english and have booth a mediterranean climate. \n",
    "\n",
    "This represent a simple way to view similarity as:\n",
    "\n",
    "    Similarity = 1 if X.attribute = Y.attribute        (Where X, Y are two objects)\n",
    "    Similarity = 0 if X.attribute ≠ Y.attribute\n",
    "\n",
    "Implent a function called **equal_attribute(x , y, attribute)**, that returns True if two cities have the same the attribute value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7B2Cf39dkYp-"
   },
   "outputs": [],
   "source": [
    "# TO DO\n",
    "def equal_attribute(x,y,attribute):\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4mTHaa1kYp-"
   },
   "outputs": [],
   "source": [
    "# Does Perth and SanFrancisco have same language? \n",
    "equal_attribute(perth_details,sanfrancisco_details,'Language')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbWloFdkkYp-"
   },
   "source": [
    "Expected output : True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjNkoRnOkYp-"
   },
   "source": [
    "Imagine if we define the similary level as the number of times a attribute is equal betwen two cities. This is a defenition based on qualitative attributes of the city details data.\n",
    "\n",
    "Implement a function **get_city_similarity_level(x ,y )**, that receives two cities details as inputs and returns the level of similary betwen them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZrQAG_ekYp-"
   },
   "outputs": [],
   "source": [
    "## TO DO\n",
    "def get_city_similarity_level(x,y):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFb0r0YqkYp-"
   },
   "source": [
    "Using the previous function display the level of similarty of Johannesburg to the other cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYq8RyBGkYp-"
   },
   "outputs": [],
   "source": [
    "print (\"Johannesburg Similarity Level to:\")\n",
    "\n",
    "print(\"Bern:\",get_city_similarity_level(johannesburg_details, bern_details) )\n",
    "print(\"San Francisco:\",get_city_similarity_level(johannesburg_details, sanfrancisco_details) )\n",
    "print(\"Perth:\",get_city_similarity_level(johannesburg_details, perth_details) )\n",
    "print(\"Brasilia:\",get_city_similarity_level(johannesburg_details, brasilia_details) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZvL7ffakYp-"
   },
   "source": [
    "## Task 4.1.3 - Proximity Measures\n",
    "In the previous task, we define a very simple similarity measure based on qualitative attributes.\n",
    "\n",
    "However, in most cases, our attributes are quantitative, as is the of our weather data.\n",
    "The similarity in this context is usually described as a distance with dimensions representing features of the objects. If this distance is small, there will be a high degree of similarity; if this distance is large, there will be a low degree of similarity.\n",
    "\n",
    "The most common example is the Euclidian distance:\n",
    "\n",
    "**Euclidian distance**\n",
    "\n",
    "$ d(i,j) =\\sqrt{(x_{i1} - x_{j1})² + (x_{i2} - x_{j2})² +(x_{id} - x_{jd})²}$\n",
    "\n",
    "where $d(i,j)$ is a distance metric between attributes of samples $i$ and $j$.\n",
    "\n",
    "Implement a function **euclidian_distance(x, y)**, that return a euclidean distance between two data series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwcXc19vkYp-"
   },
   "outputs": [],
   "source": [
    "#TO DO\n",
    "# Implement euclidian distance function\n",
    "from math import *\n",
    "def euclidian_distance(x,y):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1SxYh3K0kYp-"
   },
   "outputs": [],
   "source": [
    "#Check function\n",
    "euclidian_distance([0,1,2,0],[1,4,2,5]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUwfBU_pkYp-"
   },
   "source": [
    "Expected ouput: 5.916079783099616"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huyVavjAkYp-"
   },
   "source": [
    "Display  euclidian distance betwen temperature in Johannesburg and the other cities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mMSTRuBfkYp-"
   },
   "outputs": [],
   "source": [
    "print (\"Johannesburg Temperature euclidian distance to:\")\n",
    "print (\"Perth:\", euclidian_distance(johannesburg_weather.temp,perth_weather.temp) )\n",
    "print (\"Bern:\",euclidian_distance(johannesburg_weather.temp,bern_weather.temp) )\n",
    "print (\"San Francisco:\",euclidian_distance(johannesburg_weather.temp,sanfrancisco_weather.temp) )\n",
    "print (\"Brasilia:\", euclidian_distance(johannesburg_weather.temp,brasilia_weather.temp) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CfbDhv4kYp-"
   },
   "source": [
    "We see that closest city in temperature to Johannesburg is Perth, and the frasthest is Bern.\n",
    "This means the temperature is is more similar to Johannesburg.\n",
    "\n",
    "### Plot the Temperature\n",
    "We can cofirm our proximity meseaure by plotting the temperature of the cities.\n",
    "\n",
    "As expected it can be observed temperatue in Perth over the year is considerabily more similar to Jhoannesburg that the temperature in Bern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7kruevxkYp-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3.3 plot and compare the temperature between original data and replaced data\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(15, 5))\n",
    "ax1.set_title('Perth & Johannesburg - Similar Temperatures')\n",
    "ax1.plot(johannesburg_weather.temp, label=\"Johannesburg\")\n",
    "ax1.plot(perth_weather.temp, label=\"Perth\")\n",
    "ax1.set_ylabel(r'Temperature ($^\\circ$C)')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.set_title('Bern & Johannesburg -- Not Simimilar Temperatures')\n",
    "ax2.plot(johannesburg_weather.temp, label=\"Johannesburg\")\n",
    "ax2.plot(bern_weather.temp, label=\"Bern\")\n",
    "ax2.set_ylabel(r'Temperature ($^\\circ$C)')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8vOGPGBkYp-"
   },
   "source": [
    "Now we want to define a simililary metric between the weather in two cities. In this case we want the similary between not only the temperature, but also the precepitation, wind, etc, all the variables that constitute the weather.\n",
    "\n",
    "Write a **weather_similarty(city1, city2)** function that return the weather similarty between two cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zCndAHjYkYp_"
   },
   "outputs": [],
   "source": [
    "#TO DO\n",
    "def get_weather_similarty(city1, city2):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMOHHxu1kYp_"
   },
   "source": [
    "Use your new function to display the weather similarly of Perth to the other cities.\n",
    "\n",
    "Which city has more similar weather? and more different?  Does our weather similarity match the climate in the cities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CkzjLl67kYp_"
   },
   "outputs": [],
   "source": [
    "print (\"Perth Weather Similarity to:\")\n",
    "print (\"Bern\", get_weather_similarty(perth_weather,bern_weather) )\n",
    "print (\"San Francisco\", get_weather_similarty(perth_weather,sanfrancisco_weather) )\n",
    "print (\"Johannesburg\", get_weather_similarty(johannesburg_weather,perth_weather) )\n",
    "print (\"Brasilia\", get_weather_similarty(perth_weather,brasilia_weather) )\n",
    "\n",
    "\n",
    "CityDataset[['City Name','Climate']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIRbqnEKkYp_"
   },
   "source": [
    "Care should be taken when calculating distance across dimensions/features that are unrelated. The relative values of each feature must be normalized, or one feature could end up dominating the distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPEeG3-OkYp_"
   },
   "source": [
    "### Other Proximity Measures\n",
    "\n",
    "Besides the Euclidan distance, there are lot of other similarity distance measures, like:\n",
    "\n",
    "**Cosine Similarity:**\n",
    "\n",
    "It is defined as the cosine of the angle between two vectors.\n",
    "The cosine similarity is advantageous because even if the two similar data series are far apart by the Euclidean distance, chances are they may still be oriented closer together. The smaller the angle, higher the cosine similarity.\n",
    "\n",
    "$ Cos \\theta = \\frac{u .v}{||u||.||v||}$,\n",
    "\n",
    "where $u,v$ are two vectors projected in a multi-dimensional space. \n",
    "\n",
    "**Manhatam distance**\n",
    "\n",
    "Manhattan distance is a metric in which the distance between two points is the sum of the absolute differences of their Cartesian coordinates.\n",
    "\n",
    "$ |x_{i1} - x_{j1}| + |x_{id} - x_{jd}|  $\n",
    "\n",
    "\n",
    "You can **scipy.spatial.distance** library to import some of this meseaures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HmQVYxZOkYp_"
   },
   "outputs": [],
   "source": [
    "x = [3,4,5,6]\n",
    "y = [4,5,6,7]\n",
    "\n",
    "#Cosine distance\n",
    "from scipy.spatial.distance import cosine\n",
    "print(\"Cosine distance\", cosine(x,y) )\n",
    "\n",
    "#Manhattan distance\n",
    "from scipy.spatial.distance import cityblock\n",
    "print(\"Manhattan distance\", cityblock(x,y) )\n",
    "\n",
    "#Euclidian distance\n",
    "from scipy.spatial.distance import euclidean\n",
    "print(\"Euclidian distance\", euclidean(x,y) )\n",
    "\n",
    "#Minkowski distance - Generalisation of the Euclidan and Manhattan\n",
    "from scipy.spatial.distance import minkowski\n",
    "print(\"Minkowski distance\", minkowski(x,y) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQM0uACkkYp_"
   },
   "source": [
    "## Task 4.1.4  - Data Relationships \n",
    "\n",
    "Besides similarly, there are multiple relationships in our data. To find these relations in the data is a cornerstone in the understanding of the many concepts and methods in pattern recognition and statistics.\n",
    "\n",
    "Variables within a dataset can be related for lots of reasons, for example:\n",
    "\n",
    "-One variable could cause or depend on the values of another variable.\n",
    "\n",
    "-One variable could be lightly associated with another variable.\n",
    "\n",
    "-Two variables could depend on a third unknown variable.\n",
    "\n",
    "Therefore to study the statistical relationship between two variables we define convariance and correlation.\n",
    "\n",
    "### Convariance \n",
    "\n",
    "Covariance is a measure of how much two random variables vary together (e.g the temperature and precipitation in a city). In the covariance matrix the off-diagonal elements contain the covariances of each pair of variables. The diagonal elements of the covariance matrix contain the variances of each variable. \n",
    "\n",
    "    If COV(xi, xj) = 0 then variables are uncorrelated.\n",
    "    If COV(xi, xj) > 0 then variables positively correlated \n",
    "    If COV(xi, xj) < 0 then variables negatively correlated \n",
    "\n",
    "If the covariance is positive then the variables grow together, while a negative covariance means they move inversely. A null value means variables are independent.\n",
    "\n",
    "\n",
    "### Variance\n",
    "Variance measures the variation of a single random variable (e.g temperature in a city), how much the data are scattered about the mean.\n",
    "The variance is equal to the square of the standard deviation.\n",
    "\n",
    "### Correlation\n",
    "\n",
    "While covariance indicates the direction of the linear relationship between variables, correlation measures both the strength and direction of the linear relationship between two variables. correlation is a function of covariance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48h3No9okYp_"
   },
   "source": [
    "Let's start by plotting the precipitation in the city of Brasilia with respect to the cloud cover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vsfFBLBbkYp_"
   },
   "outputs": [],
   "source": [
    "#plot precipitation for cloud cover\n",
    "x = brasilia_weather['cloud cover']\n",
    "y = brasilia_weather['precipitation']\n",
    "plt.scatter(x,y)\n",
    "plt.title('Brasilia')\n",
    "plt.ylabel('Precipitation ($mm$)')\n",
    "plt.xlabel('Cloud cover ($oktas$)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xl1M5chnkYp_"
   },
   "source": [
    "As expected, it can be observed the data is highly correlated. Rain and clouds are natural phenomenons that generally speaking occur simultaneously.\n",
    "\n",
    "Use the NumPy function **np.cov(x, y)** , to determine the covariance between precipitation and cloud cover in the city of Brasilia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQPpsdiEkYp_"
   },
   "outputs": [],
   "source": [
    "#TO DO \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_p596fHhkYp_"
   },
   "source": [
    "Display the calculated covariance in a heatmap (use **seaborn.heatmap**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N827st0dkYp_"
   },
   "outputs": [],
   "source": [
    "#TO DO\n",
    "import seaborn as sn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnyzx8xCkYp_"
   },
   "source": [
    "What about temperature and wind speed?  Is there a correlation between the two, in a city like Brasilia?\n",
    "\n",
    "Plot the scatter graph and calculate the covariance between precipitation and temperature in the city of Brasilia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBCykvGskYp_"
   },
   "outputs": [],
   "source": [
    "#TO DO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xs57O-gMkYp_"
   },
   "outputs": [],
   "source": [
    "#TO DO \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wb7I_mZxkYp_"
   },
   "source": [
    "### Pearson's Correlation\n",
    "\n",
    "The Pearson correlation coefficient can be used to summarize the strength of the linear relationship between two data samples.\n",
    "\n",
    "### Spearman’s Correlation\n",
    "Two variables may be related by a nonlinear relationship, such that the relationship is stronger or weaker across the distribution of the variables.\n",
    "\n",
    "Further, the two variables being considered may have a non-Gaussian distribution. In this case, the Spearman’s correlation coefficient (named for Charles Spearman) can be used to summarize the strength between the two data samples. This test of relationship can also be used if there is a linear relationship between the variables, but will have slightly less power (e.g. may result in lower coefficient scores).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zy0rHDsskYp_"
   },
   "source": [
    "Use **scipy.stats import pearsonr** and **scipy.stats import spearmanr**  to calculate a correlation between two variables of your choice.\n",
    "Interpret the values, plot the variables to confirm your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CuMw73D6kYp_"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "#TO DO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-LpPIclRkYp_"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "#TO DO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r77PCrhqkYp_"
   },
   "source": [
    "## Task 4.1.4 - Hypothesis Testing\n",
    "\n",
    "A common problem in applied machine learning is determining whether input features are relevant to the outcome to be predicted. This is the problem of feature selection, we saw in the previous taks that convariance and correlation can be used to learn relations between quantitative data.\n",
    "\n",
    "However if instead of having temperature mesaures, we only have describtion if the day was cold or hot. This is called a categorical variable, it is a variable that can take on one of a limited, and usually fixed, number of possible values.\n",
    "\n",
    "In the case of classification problems where input variables are also categorical. \n",
    "We can use statistical tests to determine whether the output variable is dependent or independent of the input variables. If independent, then the input variable is a candidate for a feature that may be irrelevant to the problem and removed from the dataset.\n",
    "\n",
    "## Task 4.1.4.1 - Pearson’s Chi-squared\n",
    "The Pearson’s Chi-squared statistical hypothesis is an example of a test for independence between categorical variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-dYCcx7kYp_"
   },
   "source": [
    "### Categorical Data\n",
    "\n",
    "In this task, instead of having our precise weather measures, we only have the observations of an ordinary person.\n",
    "This person only destiguied the days as hot or cold, and for a full year he counted the number days it rained, was cloud or sunny.\n",
    "\n",
    "This results in a table with **Qualitative attributes** more precisely **nominal attributes**\n",
    "\n",
    "Run the following code and display the table CaliTable with days counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vi20SssfkYp_"
   },
   "outputs": [],
   "source": [
    "#@Helper Function\n",
    "#Counting the hot/cold days and if it rains or not\n",
    "\n",
    "def count_weather_days(city_weather):\n",
    "    \n",
    "    #Downsample by max value in a day\n",
    "    city_weather_day = city_weather.resample('D').max()\n",
    "\n",
    "    rainy_days = len(city_weather_day[(city_weather_day['precipitation']> 2 )])   \n",
    "    cloudy_days = len(city_weather_day[(city_weather_day['precipitation']> 0.0001) & (city_weather_day['precipitation']<= 1)])                    \n",
    "    sunny_days = len(city_weather_day[(city_weather_day['precipitation']<= 0.0001)])   \n",
    "    \n",
    "    hot_days = len(city_weather_day[(city_weather_day['temp']>17)])   \n",
    "    cold_days = len(city_weather_day[(city_weather_day['temp']<= 17)])  \n",
    "    \n",
    "    hot_sunny_days = len(city_weather_day[(city_weather_day['temp']>17) & \n",
    "                                                 (city_weather_day['precipitation']<= 0.0001) ])   \n",
    "    cold_sunny_days = len(city_weather_day[(city_weather_day['temp'] <= 17) & \n",
    "                                                 (city_weather_day['precipitation']<= 0.0001) ])\n",
    "    \n",
    "    hot_rainy_days = len(city_weather_day[(city_weather_day['temp']>17) & \n",
    "                                                 (city_weather_day['precipitation']> 2 ) ]) \n",
    "    cold_rainy_days = len(city_weather_day[(city_weather_day['temp']<= 17) & \n",
    "                                                 (city_weather_day['precipitation']> 2 ) ])\n",
    "\n",
    "    hot_cloudy_days = hot_days - hot_rainy_days - hot_sunny_days\n",
    "    cold_cloudy_days = cold_days - cold_rainy_days -cold_sunny_days\n",
    "    \n",
    "    Sunny = [hot_sunny_days,cold_sunny_days]\n",
    "    Cloudy = [hot_cloudy_days,cold_cloudy_days]\n",
    "    Rainy = [hot_rainy_days,cold_rainy_days ]\n",
    "    Total =[hot_days, cold_days]\n",
    "\n",
    "    columns = ['Hot Days','Cold days']\n",
    "    index = ['Hot Days','Cold days']\n",
    "    d = {'Sunny days': Sunny, 'Cloudy days': Cloudy, 'Rainy days': Rainy }\n",
    "    \n",
    "    Table_Simple =pd.DataFrame(data=d,index=index,columns=None)\n",
    "    \n",
    "    Sunny = [hot_sunny_days,cold_sunny_days, sunny_days]\n",
    "    Cloudy = [hot_cloudy_days,cold_cloudy_days, cloudy_days]\n",
    "    Rainy = [hot_rainy_days,cold_rainy_days, rainy_days ]\n",
    "    Total =[hot_days, cold_days, (hot_days+ cold_days)]\n",
    "\n",
    "    columns = ['Hot Days','Cold days', 'Total']\n",
    "    index = ['Hot Days','Cold days', 'Total']\n",
    "    d = {'Sunny days': Sunny, 'Cloudy days': Cloudy, 'Rainy days': Rainy, 'Total': Total }\n",
    "    Table_with_Total = pd.DataFrame(data=d,index=index,columns=None)\n",
    "    \n",
    "    return (Table_Simple, Table_with_Total )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QHdA_bI1kYp_"
   },
   "outputs": [],
   "source": [
    "CaliTable, CaliTable_with_Total = count_weather_days(sanfrancisco_weather)\n",
    "\n",
    "CaliTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XX_9RBQlkYp_"
   },
   "source": [
    "\n",
    "|           \t| Sunny days \t| Cloudy days \t| Rainy days \t| Total \t|\n",
    "|:---------:\t|:----------:\t|:-----------:\t|:----------:\t|:-----:\t|\n",
    "|  Hot Days \t|     205    \t|      7      \t|      4     \t|  216  \t|\n",
    "| Cold days \t|     95     \t|      25     \t|     29     \t|  149  \t|\n",
    "|   Total   \t|     300    \t|      19     \t|     33     \t|  365  \t|\n",
    "\n",
    "\n",
    "Nice and warm San Francisco with 300 sunny days, and only 149 colds days a year. \n",
    "\n",
    "A table summarization of two categorical variables in this form is called a contingency table.\n",
    "The table was called a contingency table because the intent is to help determine whether one variable is contingent upon or depends upon the other variable. \n",
    "\n",
    "For example, does a hot day (temperature) depend on good weather( rain/sun), or are they independent?\n",
    "Can you answer that from the table alone? To cofirm, we will use Pearson’s Chi-Squared test.\n",
    "\n",
    "Pearson's chi-squared test is used to determine whether there is a statistically significant difference between the expected frequencies and the observed frequencies in one or more categories of a contingency table. \n",
    "\n",
    "\n",
    "In this example, the number of observations for a category (hot or cold) is not the same. Nevertheless, we can calculate the expected frequency of observations, determine whether there is a statistically significant difference between the expected frequencies and the observed frequencies in one or more categories of a contingency table. \n",
    "\n",
    "In the standard applications of this test, the observations are classified into mutually exclusive classes. The result of the test can be interpreted to reject or fail to reject the assumption or null hypothesis that the observed and expected frequencies are the same.\n",
    "\n",
    "If the null hypothesis is that there are no differences between the classes in the population is true, the test statistic computed from the observations follows a chi-squared ($χ2$) frequency distribution. The purpose of the test is to evaluate how likely the observed frequencies would be assuming the null hypothesis is true. \n",
    "The variables are considered independent if the observed and expected frequencies are similar\n",
    "\n",
    "\n",
    "We will use **chi2_contingency() SciPy function** that takes as input representing the contingency table for the two categorical variables, and returns the calculated statistic and p-value for interpretation as well as the calculated degrees of freedom and table of expected frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UbL39c5hkYp_"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import chi2\n",
    "\n",
    "stat, p, dof, expected = chi2_contingency(CaliTable)\n",
    "print(\"statistic\",stat)\n",
    "print(\"p-value\",p)\n",
    "print(\"degres of fredom: \",dof)\n",
    "print(\"table of expected frequencies\\n\",expected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lX1hWcXckYp_"
   },
   "source": [
    "### Interpret the test-statistic\n",
    "\n",
    "For example, if we assume the variable is independent, with a probability of 90%, suggesting that the finding of the test is quite likely. If the statistic is less than or equal to the critical value, we can fail to reject this assumption, otherwise it can be rejected.\n",
    "\n",
    "    If p-value <= critical: significant result, reject null hypothesis (H0), dependent.\n",
    "    If p-value > critical: not significant result, fail to reject null hypothesis (H0), independent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fpnhPp-NkYp_"
   },
   "outputs": [],
   "source": [
    "prob = 0.90\n",
    "critical = chi2.ppf(prob, dof)\n",
    "if abs(stat) >= critical:\n",
    "    print('Dependent (reject H0)')\n",
    "else:\n",
    "    print('Independent (fail to reject H0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFYbMoTrkYp_"
   },
   "source": [
    "We can also interpret the p-value by comparing it to a chosen significance level, which would be 10%, calculated by inverting the probability used in the critical value interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_nj_XOVkYp_"
   },
   "outputs": [],
   "source": [
    "# interpret p-value\n",
    "alpha = 1.0 - prob\n",
    "if p <= alpha:\n",
    "    print('Dependent (reject H0)')\n",
    "else:\n",
    "    print('Independent (fail to reject H0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWJgdQCDkYp_"
   },
   "source": [
    "The  Chi-Square Test confirms that the variables are dependent."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LAB4.1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
